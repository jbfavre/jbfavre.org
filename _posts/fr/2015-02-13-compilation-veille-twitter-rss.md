---
layout: post
title: "Compilation veille Twitter & RSS #2015-06"
category: veille
---

La moisson de liens pour les semaine du 9 au 13 janvier 2015.
Ils ont, pour la plupart été publiés sur mon compte Twitter.
Les voici rassemblés pour ceux qui les auraient raté.

Bonne lecture

## Big Data

[Operationalizing Spark with MemSQL](http://blog.memsql.com/operationalizing-spark-with-memsql/)
:  Combining the data processing prowess of Spark with a real-time database for transactions and analytics, where both are memory-optimized and distributed, leads to powerful new business use cases. MemSQL Spark Connector links at end of this post.

[How-to: Install and Use Cask Data Application Platform Alongside Impala](http://blog.cloudera.com/blog/2015/02/how-to-install-and-use-cask-data-application-platform-alongside-impala/)
: Today, Cloudera and Cask are very happy to introduce the integration of Cloudera’s enterprise data hub (EDH) with the Cask Data Application Platform (CDAP). CDAP is an integrated platform for developers and organizations to build, deploy, and manage data applications on Apache Hadoop. This initial integration will enable CDAP to be installed, configured, and managed from within Cloudera Manager, a component of Cloudera Enterprise. Furthermore, it will simplify data ingestion for a variety of data sources, as well as enable interactive queries via Impala. Starting today, you can download and install CDAP directly from Cloudera’s downloads page.

[New in Cloudera Manager 5.3: Easier CDH Upgrades](http://blog.cloudera.com/blog/2015/02/new-in-cloudera-manager-5-3-easier-cdh-upgrades/)
: Upgrades can be hard, and any downtime to mission-critical workloads can have a direct impact on revenue. Upgrading the software that powers these workloads can often be an overwhelming and uncertain task that can create unpredictable issues. Apache Hadoop can be especially complex as it consists of dozens of components running across multiple machines. That’s why an enterprise-grade administration tool is necessary for running Hadoop in production, and is especially important when taking the upgrade plunge.

[Run Real-Time Applications with Spark and the MemSQL Spark Connector](http://blog.memsql.com/memsql-spark-connector/)
: Apache Spark is one of the most powerful distributed computing frameworks available today. Its combination of fast, in-memory computing with an architecture that’s easy to understand has made it popular for users working with huge amounts of data.

[Couchdoop: Couchbase Meets Apache Hadoop](http://blog.cloudera.com/blog/2015/02/couchdoop-couchbase-meets-apache-hadoop/)
: Couchdoop is a Couchbase connector for Apache Hadoop, developed by Avira on CDH, that allows for easy, parallel data transfer between Couchbase and Hadoop storage engines. It includes a command-line tool, for simple tasks and prototyping, as well as a MapReduce library, for those who want to use Couchdoop directly in MapReduce jobs. Couchdoop works natively with CDH 5.x.

[Data Processing with Apache Crunch at Spotify](http://blog.cloudera.com/blog/2015/02/data-processing-with-apache-crunch-at-spotify/)
: All of our lovely Spotify users generate many terabytes of data every day. All the songs that are listened to, all the playlists you make, all the people you follow, and all the music you share. Somehow we need to organize, process, and aggregate all of this into meaningful information out the other side. Here are just a few of the things we need to get out of the data

[4 Steps for Structuring Your Log Data](https://blog.logentries.com/2015/02/4-steps-for-structuring-your-log-data/)
: In the age of BigData we are taught that no pile of data is too large or too complex. This is absolutely true. Most data analysis systems can take any type and volume of data — but ingestion is much different from consumption. The way your data is structured directly impacts its ability to be consumed, understood, and correlated with other data. Here are the top 4 ways to make sure your system and app logs help you to do this effectively.

[Understanding the Google Analytics Cohort Report](http://cutroni.com/blog/2015/02/06/using-cohort-report/)
: A very common data analysis technique is called Cohort Analysis.  
A Cohort is simply a segment of users which is based on a date. For example, a cohort could be all users based on their Acquisition Date (in Google Analytics this is really the Date of First Session).

### Analytics

[What Data Nerds Say About the Future of Data and Apps](http://blog.newrelic.com/2015/02/10/datanerd-survey-comments/)
: Late last year, New Relic conducted a survey on what people thought would be important about big data in 2015, and hundreds of data nerds responded. We shared some of their Twitter responses in January, and last week we posted an awesome infographic and an analysis based on survey results predicting that 2015 would be the Year of the Data Nerd.

[New Relic Runs on Insights: Maximizing Engineering Resources](http://blog.newrelic.com/2015/02/10/insights-engineering-performance-usage/)
: This New Relic Insights use case is taken from New Relic Runs on Insights, a new whitepaper collecting Insights use cases from across New Relic, highlighting some of the many ways the product can help businesses exploit the countless possibilities for leveraging their data in real time, without needing developer resources, and allowing them to make faster, more informed decisions. Download the New Relic Runs on Insights whitepaper now!

[Forecasting Beer Consumption with Sklearn](http://python.dzone.com/articles/forecasting-beer-consumption)
: In this post we will see how to implement a straightforward forecasting model based on the linear regression object of sklearn. The model that we are going to build is based on the idea idea that past observations are good predictors of a future value. Using some symbols, given xn−k,...,xn−2,xn−1 we want to estimate xn+h where h is the forecast horizon just using the given values. The estimation that we are going to apply is the following

## Management

[The DevOps Identity Crisis](http://www.xaprb.com/blog/2015/02/07/devops-identity-crisis/)
: DevOps is everywhere! The growth and mindshare of the movement is remarkable. But if you care deeply about DevOps, you might agree with me when I say that although its moment has “arrived,” DevOps is in serious trouble. The movement is fragmented and weakly defined, and is being usurped by those who care more about short-term opportunities than the long-term viability of DevOps.

[Time To Up The Accountability Of Your Agile Teams](http://architects.dzone.com/articles/time-accountability-your-agile)
: In my last entry, I talked a bit about what accountability means to me. Check it out to create a shared understanding between us before reading on…  
In this continuation, I will begin to dive into structures that support accountability at scale. For now, there seems to be enough material at the scrum team level and then subsequently at the scaled level. Here’s the delivery team level for ya.

[The Real Cost of Downtime, The Real Potential of DevOps](http://java.dzone.com/articles/real-cost-downtime-real-potential-devops)
: Hang around any IT department (or the AppDynamics offices) and you likely won’t finish a cup of coffee before somebody brings up the cost of downtime or the ascent of DevOps. And that’s a good thing, because these are two topics that are central to the value that IT brings to enterprises today and how that value will increase in the future.

[Building Effective API Programs: API Business Models](http://java.dzone.com/articles/building-effective-api)
: As discussed in the previous post, an API program must be embedded in the overarching business strategy and aligned with business goals. The business model is where the rubber meets the road here — how this API, or these APIs, will interact with existing resources, activities, and partnerships as well the the required cost structure.

[Automated Testing Shows a Respect for Employees](http://java.dzone.com/articles/automated-testing-shows)
: In the tech-artists.org G+ community page there was a comment on a thread about unit testing

## Architecture

[All about Apache Aurora](https://blog.twitter.com/2015/all-about-apache-aurora)
: Today we’re excited to see the Apache Aurora community announce the 0.7.0 release. Since we began development on this project, Aurora has become a critical part of how we run services at Twitter. Now a fledgling open source project, Aurora is actively developed by a community of developers running it in production.

[Vinted Architecture: Keeping a busy portal stable by deploying several hundred times per day](http://highscalability.com/blog/2015/2/9/vinted-architecture-keeping-a-busy-portal-stable-by-deployin.html)
: Vinted is a peer-to-peer marketplace to sell, buy and swap clothes. It allows members to communicate directly and has the features of a social networking service.

[Linked partitioning to replace offset-based pagination](https://developers.soundcloud.com/blog/offset-pagination-deprecated)
: The SoundCloud API will be dropping support for offset-based pagination on March 2, 2015, in favor of linked partitioning.

[Smart Endpoints. Smart Pipes. Smarter Microservices.](http://blog.iron.io/2015/02/smart-endpoints-smart-pipes-smarter.html)
: This is the second post in a series on microservices. Read our previous post on the service computing environment: The Ephemeral Life of Dockerized Microservices.  
The use of microservices is a growing trend in application development because it provides an answer to the overhead incurred by monolithic applications. The practice of breaking apart application components into independent services introduces more moving parts to manage, however, requiring a robust communication method in order to keep services and data in sync. The idea is to decouple services without derailing your application. 

[Surviving at Scale: Lessons from Airbnb and LinkedIn](http://nerds.airbnb.com/surviving-scale-lessons-airbnb-linkedin/)
: We are super excited to host our first tech talk of 2015! With the new year comes a new schedule of tech talks happening once a month. Each talk will be carefully curated with multiple speakers and a cohesive theme. The theme this month is scaling and infrastructure with Airbnb engineers, Ben Hughes and Jon Tai and the co-founder and head of engineering at Confluent, Neha Narkhede.

[Achieving 100% Uptime and 139% Increase in Holiday Online Sales: Lessons Learned](http://natishalom.typepad.com/nati_shaloms_blog/2015/02/achieving-100-uptime-and-139-increase-in-holiday-online-sales-lessons-learned.html)
: You don’t need to be an expert to realize that a failure of an eCommerce site during Black Friday or Cyber Monday is a disastrous event, leading to huge loss in revenue and reputation for the retailer. As the share of eCommerce accounts increases to more than 8% of total US retail sales this year, the impact of failure becomes more significant - not just to the site itself, but on the the overall economy. A study on the subject, compiled by Joyent and New Relic, showed that 86% of companies experienced one or more episodes of downtime last holiday season. At the same time, 58% of customers will not use a company’s site again after experiencing site errors.

### HTTP/2

[
[Service Side Push in HTTP/2 With nghttp2](http://ma.ttias.be/service-side-push-http2-nghttp2/)
: At this pace of development, nghttp2 is a project to keep an eye on.

[Hello HTTP/2, Goodbye SPDY](http://blog.chromium.org/2015/02/hello-http2-goodbye-spdy-http-is_9.html)
:  HTTP is the fundamental networking protocol that powers the web. The majority of sites use version 1.1 of HTTP, which was defined in 1999 with RFC2616. A lot has changed on the web since then, and a new version of the protocol named HTTP/2 is well on the road to standardization. We plan to gradually roll out support for HTTP/2 in Chrome 40 in the upcoming weeks.

## Ops

[Chef-12 Dynamic Resource and Provider Resolution](https://www.chef.io/blog/2015/02/10/chef-12-provider-resolver/)
:  In Chef 12 the old Chef::Platform hashmap located in `lib/chef/platform/provider_mapping.rb` has been deprecated. In its place is a dynamic provider and resolver resolution mechanism which is preferred and which can be manipulated via DSL methods on the resource and provider. In Chef 11 it was common to add functionality for platforms in the Chef::Platform hashmap which looks like this

[Chef Quick Tip: Create a Provisioner Node](http://jtimberman.housepub.org/blog/2015/02/09/quick-tip-create-a-provisioner-node/)
: This quick tip is brought to you by my preparation for my ChefConf talk about using Chef Provisioning to build a Chef Server Cluster, which is based on my blog post about the same. In the blog post I used chef-zero as my Chef Server, but for the talk I’m using Hosted Chef.

## Dev

[Scaling Vehicle Routing with Nearby Selection](http://java.dzone.com/articles/scaling-vehicle-routing-nearby)
: OptaPlanner 6.2 (an open source Java constraint satisfaction engine) has made a big step forward for the Vehicle Routing Problem (VRP), Traveling Salesman Problem (TSP) and similar use cases. The new feature nearby selection enables it to scale to bigger problems much more efficiently without sacrificing potential optimal solutions (which is common for inferior techniques).

[Algorithms Explained: Minesweeper](http://java.dzone.com/articles/minesweeper-algorithms-explained)
: This blog post explains the essential algorithms for the well-known Windows game "Minesweeper."

[Easily working with multiple repositories (git, hg, etc)](http://www.debian-administration.org/article/713/Easily_working_with_multiple_repositories_git_hg_etc)
: There are situations where it is ueful to checkout multiple repositories, which might use different revision control systems, and operate upon them as a groupapt. This is what the mr tool was designed for.

### Python

[Python: Find the Highest Value in a Group](http://python.dzone.com/articles/python-find-highest-value)
: In my continued playing around with a "How I Met Your Mother" data set I needed to find out the last episode that happened in a season so that I could use it in a chart I wanted to plot.

[Constant Rebalanced Portfolios - some simulations with numpy](https://www.chrisstucchio.com/blog/2015/constant_rebalanced_portfolios.html)
: I've had several discussions with friends lately about investing, mostly in the wake of robinhood.io's early release. Quite a few people have the plan to put some cash into robinhood.io accounts and engage in some sort of investment strategy. Strategy ideas range from daytrading (I do NOT recommend this) to buy&hold. One notable strategy that not a single person has discussed is the constant rebalanced portfolio, and I think it's important to get word of such theories out there.

### Android

[Android: Unit Testing Apps with Couchbase, Robolectric and Dagger](http://java.dzone.com/articles/android-unit-testing-apps)
: I need a database for my TripComputer app so that users can keep a log of their Journeys. I could use SQL Lite, but I prefer not to use SQL if possible. With SQL you’re forced to maintain a fixed schema and SQL Lite doesn’t offer any out of the box cloud replication capabilities, unlike most NoSQL databases.

## Databases

[Introducing the Database Selection Matrix](http://java.dzone.com/articles/introducing-database-selection)
: For the better part of a generation, the database landscape had changed very little. No one could say “this is not your father’s database.” They had become, in a word, boring.  
Then a combination of factors catalyzed an era of innovation in database technologies: cheap storage and compute resources; pervasive connectivity; social networks; smartphones; the proliferation of sensors; open source software. Data volumes grew (and are growing) at exponential rates. Over 80% of today’s data no longer fits neatly into the normalized row and column table formats of the past. And so developers began engineering solutions to a new set of problems with a very different set of resources and assumptions. Today these new options include a variety of database architectures built around diverse data models – from key-value to document to wide-column and graph. And of course you still have the option of the venerable relational database.

### TokuDB

[TokuDB Table Optimization Improvements](http://www.tokutek.com/2015/02/tokudb-table-optimization-improvements/)
:  Tokutek’s fractal tree technology provides fast performance by injecting small messages into buffers inside the fractal tree index. This allows writes to be batched, thus eliminating io that is required in traditional btree indexes for every operation. Additional background information on how fractal trees operate can be found in Zardosht Kasheff’s blog entitled, TokuMX Fractal Tree Indexes, What Are They? Don’t be thrown off by the title, Fractal Tree Indexes access data in the same way for TokuDB as they do for TokuMX.

### MySQL

[Everything You Need to Know About Scaling MySQL – Part 6: Aurora](http://blog.clustrix.com/2015/02/11/everything-need-know-scaling-mysql-part-6-aurora/)
: In November of last year, Amazon announced a new MySQL-compatible scale up relational database engine called Aurora. The online retailer billed the new product as highly scalable, boasting its capability to automatically grow storage from 10GB to 64TB based on need.

[Online GTID rollout now available in Percona Server 5.6](http://www.percona.com/blog/2015/02/10/online-gtid-rollout-now-available-percona-server-5-6/)
: Global Transaction IDs (GTIDs) are one of my favorite features of MySQL 5.6. The main limitation is that you must stop all the servers at the same time to allow GTID-replication. Not everyone can afford to take a downtime so this requirement has been a showstopper for many people. Starting with Percona Server 5.6.22-72.0 enabling GTID replication can be done without almost no downtime. Let’s see how to do it.

[Writing MySQL Purge Jobs in Ruby with the Cleansweep Gem](http://java.dzone.com/articles/writing-mysql-purge-jobs-ruby)
: About the simplest thing you can do with a relational database is tell it to delete some data:  
`delete from accounts where accounts.cancelled = true`  
You don’t have to be a DBA to figure out what that does. If only everything was that simple!

### Redis

[Advanced Redis data structures](http://amix.dk/blog/post/19756)
:  A few days ago I gave a talk at PyCon Belarus regarding advanced Redis structures.
